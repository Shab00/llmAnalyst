{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Fine-tuning a generative AI model on legal contracts and case law for enhanced contextual understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5331,
     "status": "ok",
     "timestamp": 1738183848123,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "npbdNW6xGMxb",
    "outputId": "8bdbe3f1-c968-4b9e-b5a9-5ca5046491ae"
   },
   "source": [
    "!pip install transformers scikit-learn evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19501,
     "status": "ok",
     "timestamp": 1738183879684,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "OvuVT8mMFn4C",
    "outputId": "a0282436-3616-4bb4-da28-092e6f08d149"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Modules Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16305,
     "status": "ok",
     "timestamp": 1738183912884,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "n85AaP6NGB9Q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, pipeline\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import GenerationConfig\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`pandas`**\n",
    "   - A powerful data manipulation and analysis library in Python. It provides data structures like DataFrames and Series, which are ideal for handling and analyzing structured data.\n",
    "   - **Common Usage**:\n",
    "     - Loading, cleaning, and analyzing data in various formats (CSV, Excel, JSON, etc.).\n",
    "     - Performing operations like filtering, grouping, and aggregating large datasets.\n",
    "\n",
    "2. **`torch` (from PyTorch)**\n",
    "   - A deep learning framework for building and training neural networks. It provides powerful tools for tensor computation, GPU acceleration, and automatic differentiation.\n",
    "   - **Common Usage**:\n",
    "     - Defining and training neural network models.\n",
    "     - Performing tensor operations and manipulating data for deep learning tasks.\n",
    "   - PyTorch is commonly used in conjunction with transformer models for NLP tasks.\n",
    "\n",
    "3. **`transformers` (from `Hugging Face`)**\n",
    "   - A widely used library for working with pre-trained deep learning models, especially in Natural Language Processing (NLP). It provides an easy-to-use interface for transformer-based models like BERT, GPT, T5, BART, etc.\n",
    "   - **Common Usage**:\n",
    "     - Loading, fine-tuning, and using pre-trained models for a variety of NLP tasks such as classification, summarization, question answering, and translation.\n",
    "\n",
    "   - **Submodules**:\n",
    "     \n",
    "     - **`BartTokenizer`**:\n",
    "       - The tokenizer for the BART model, responsible for preparing input text by splitting it into tokens that can be processed by the model.\n",
    "     \n",
    "     - **`BartForConditionalGeneration`**:\n",
    "       - A pre-trained BART model specifically designed for conditional generation tasks such as text summarization, translation, and text generation.\n",
    "     \n",
    "     - **`Trainer`**:\n",
    "       - A utility class that simplifies the process of training and evaluating transformer models. It handles the training loop, validation, and saving the model during the training process.\n",
    "     \n",
    "     - **`TrainingArguments`**:\n",
    "       - A class that allows you to specify various hyperparameters and settings for the training process, such as batch size, learning rate, and evaluation strategy.\n",
    "\n",
    "     - **`pipeline`**:\n",
    "       - A high-level interface for performing specific tasks like text generation, translation, sentiment analysis, and more using pre-trained models. It abstracts away much of the underlying complexity.\n",
    "\n",
    "     - **`DataCollatorForSeq2Seq`**:\n",
    "       - A data collator used to efficiently batch and pad sequences of variable length for sequence-to-sequence tasks such as summarization or translation.\n",
    "\n",
    "     - **`GenerationConfig`**:\n",
    "       - A class that manages the generation configurations for models, allowing you to fine-tune various parameters like temperature, max length, and top-k sampling during the text generation process.\n",
    "\n",
    "4. **`datasets` (from Hugging Face)**\n",
    "   - A library for easily loading, processing, and working with large datasets. It provides easy access to a variety of datasets for tasks like text classification, question answering, and summarization.\n",
    "   - **Common Usage**:\n",
    "     - Loading datasets for machine learning tasks.\n",
    "     - Preprocessing and transforming datasets into formats suitable for training or evaluation.\n",
    "\n",
    "     - **`Dataset`**:\n",
    "       - A class used to work with datasets, particularly when you need to preprocess, filter, or batch datasets in preparation for model training or evaluation.\n",
    "\n",
    "#### Summary:\n",
    "- **`pandas`** is used for data manipulation, ideal for handling structured data such as case law or other legal documents.\n",
    "- **`torch`** provides the deep learning framework for model training and tensor computation.\n",
    "- **`transformers`** is the core library for working with pre-trained models like BART, providing tools for tokenization, model generation, training, and evaluation.\n",
    "- **`datasets`** provides an easy interface for loading and working with datasets, which is useful for model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Tokenizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339,
     "referenced_widgets": [
      "0a064819a9b5454ca8c4346f4be6b33d",
      "27fc825361fb4464944bae4d4b632a11",
      "fe3c67d0ffca4472a17cd23fd600fe1d",
      "7e3bb94e179c4369bba1d19e30572d44",
      "9287cb90b96c43f099657addded0f470",
      "fe2550a58bc049db8e3a3cdff1632be4",
      "eaf9a55068e249d5b462abb566dfdd56",
      "a491bb92b7de43a79c0aaf4365eb255c",
      "d0552d1580e04b659376d778bb627d00",
      "4cb9f75e347144a4974f5f351947f1f7",
      "04e229acc5b94887823a92ea38dab2ec",
      "60be8f561df348dbaa18db0f03a124db",
      "4caaa2d47b9946e38a33f52e5ef7ccf7",
      "deef57f29f29478bad0f1f4c55bd8f68",
      "b03ce3f00b6b4f43addaf8aa2d2fb781",
      "8854674ddc16421eb7c9e2ef416bbce8",
      "1b5ff8c6e8534cc5bb0c212c88c21038",
      "14a2fe37faae430f9befb41b4f604aaa",
      "b3115262f42449da8a2912dc41f72312",
      "6cf90932e4484a8285c9b1e59a164cd6",
      "c2a5d74a0e474c9495ee5920eccdda61",
      "4b73d3678d5148fe8800e04f1a47fca0",
      "49293a70e9c249eca79490408f200374",
      "0587498b318a43718178e0a151ea01c8",
      "b40776715ddd4e138406b527d77ae955",
      "ffc5be00985e41feabf1cd85a64c8c71",
      "f7bf22a1143a4f0ebed4af5ad331e8d1",
      "26aebd9fbb3e4bdcae9eba7e870aa034",
      "a0304df7be264a86b5805da4aabc0fa1",
      "472771843d8d4924af87d3210d7f575d",
      "4e86d895b4cf4af8ab641b2072ace58c",
      "b875462794044f3c9a1f90f98cf0fa50",
      "a1fad8645b48404bbae693bf136c624a",
      "420a767135174acc82c66694580809be",
      "cb08d4587c954cb7ae3b5c49e25427a8",
      "60ad005d59d64faca9fab7d2bbd7474b",
      "f6e21b5497f642049c9c5fe366b68dd6",
      "f9b6435a8e024bdea62a953eba8f1cee",
      "b6e10e1251584a329ceda14966c608b7",
      "e85654b801fa41e9b8faeb53483cae15",
      "e24dd235d89b47daa75789643b0e85ef",
      "65c3f791c12549ecb6d9643521d34c72",
      "f48afa3f41d541c6b9c80569faff5267",
      "fb51a0b95f804d4f818b80b72a302f90",
      "3ce3b3fb9ec242738d76220868bbcc2e",
      "ac57e754692449a8adc6996dcf650b4f",
      "a1e83a3e464448f7b87c6c2878831261",
      "256d2f07de874ba4b79c2d8ccd442477",
      "f2f60d52e876472cbdb791ae14c7c51d",
      "7c89f21deb834238b3113972583cd95e",
      "08209b7e2d174dea8e28074971a72bab",
      "4bb9b4db310f43f1b78d00e80e5f7687",
      "2ea9ab874cfe43ffabb4a4684d6b925e",
      "eac9fe5bf0274e11b358d1bc1923d9c0",
      "05aa8899d66e46e8b6c61f1fe17f3de1",
      "26133c6030f448f99a01cf8ea56d8d01",
      "938cae56e68c4b858c191a34b5b69ef0",
      "314f2df3485f40f9a01d693abe20d9a4",
      "bc9932704bb24b7eb04d9dcd51337125",
      "e5abdc896e0e414f904c011e73812e63",
      "8b5c0d81da2f42cb8ee9e3d28a4169b5",
      "39df47a8ad6543a290a79783d3c8155b",
      "25d1a928584c449787ecf881b81e3044",
      "3b66e8b24bc04a24a136114320762df6",
      "806cfc7595d74865a960eafff232d1ca",
      "1884a8b47fc64fb89b9071ba8c940673"
     ]
    },
    "executionInfo": {
     "elapsed": 13557,
     "status": "ok",
     "timestamp": 1738183931489,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "7kAcpmwcUSF5",
    "outputId": "36b31041-a192-447a-c190-3ac04d1c80ee"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`model_name = \"facebook/bart-large-cnn\"`**:\n",
    "    - This sets the variable `model_name` to the identifier of the pre-trained BART model.\n",
    "    - The model `\"facebook/bart-large-cnn\"` is a large transformer model fine-tuned specifically for CNN/Daily Mail summarization tasks.\n",
    "\n",
    "- **`tokenizer = BartTokenizer.from_pretrained(model_name)`**:\n",
    "    - The `BartTokenizer` is used to load the pre-trained tokenizer for the model.\n",
    "    - The tokenizer is responsible for converting raw text into tokens that can be processed by the model. It also handles padding and truncation of text sequences.\n",
    "    - The `from_pretrained()` function loads the tokenizer associated with the pre-trained BART model, ensuring the correct tokenization for summarization.\n",
    "\n",
    "- **`model = BartForConditionalGeneration.from_pretrained(model_name)`**:\n",
    "    - This line loads the pre-trained BART model using the identifier `model_name`.\n",
    "    - The model is designed for conditional text generation tasks, such as summarization. In this case, it has been fine-tuned for CNN/Daily Mail summarization.\n",
    "    - The `from_pretrained()` function downloads and initializes the model with its pre-trained weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1576,
     "status": "ok",
     "timestamp": 1738183935825,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "ru4jj2DjF3OU"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/summarized_case_law.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1738183947905,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "oMCIxUKjuaqs",
    "outputId": "428bce18-6389-4276-bc3a-d6f2e5506be9"
   },
   "outputs": [],
   "source": [
    "df['text_length'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "print(df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1738183950633,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "jpgdswFSu46Z",
    "outputId": "34e4d1b1-a6cf-47e6-dd63-25608578d086"
   },
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['cleaned_text'].astype(str)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1738183953761,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "NwVyKWGgvbx_"
   },
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['cleaned_text'].str.replace('[^\\w\\s]', '', regex=True)\n",
    "df['cleaned_text'] = df['cleaned_text'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1738184035628,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "NZ53fg1c1lWF",
    "outputId": "a770c65e-9099-4ad7-f308-325a2ee01192"
   },
   "outputs": [],
   "source": [
    "def split_text(text, max_length=1024):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+max_length]) for i in range(0, len(words), max_length)]\n",
    "\n",
    "df['split_texts'] = df['cleaned_text'].apply(split_text)\n",
    "split_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    case_law_chunks = split_text(row['cleaned_text'], max_length=1024)\n",
    "    for chunk in case_law_chunks:\n",
    "        split_rows.append({\n",
    "            'cleaned_text': chunk,\n",
    "            'summary': row['summary']\n",
    "        })\n",
    "\n",
    "split_df = pd.DataFrame(split_rows)\n",
    "\n",
    "print(split_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`split_text(text, max_length=1024)`**:\n",
    "    - The `split_text` function is designed to break down long text into smaller, more manageable chunks. This is important when dealing with large documents, as models like BART or transformers in general have a maximum token limit (in this case, 1024 tokens).\n",
    "    - The function splits the text into individual words and then groups them into chunks of a specified maximum length (`max_length=1024`). The `max_length` parameter ensures that each chunk contains no more than 1024 words, which is a suitable size for processing by the model.\n",
    "    - **Why Split Text?**: The reason for splitting text is that many natural language models, including BART, cannot process long documents as a single input if the text exceeds the model’s token limit. By splitting long documents into smaller parts, you can ensure that the text fits within the model's constraints and avoids truncation.\n",
    "\n",
    "- **`df['split_texts'] = df['cleaned_text'].apply(split_text)`**:\n",
    "    - This line applies the `split_text` function to each entry in the `cleaned_text` column of the dataframe. The result is stored in a new column called `split_texts`, where each document is split into smaller chunks.\n",
    "    - This ensures that each chunk of text is of a size that the model can process without running into memory or token limits.\n",
    "\n",
    "- **`split_rows = []`**:\n",
    "    - An empty list `split_rows` is created to store the individual rows that will form the new dataframe.\n",
    "\n",
    "- **`for _, row in df.iterrows():`**:\n",
    "    - This loop iterates through each row of the dataframe `df`. For each row, the cleaned text (`row['cleaned_text']`) is split into smaller chunks using the `split_text` function.\n",
    "\n",
    "- **`case_law_chunks = split_text(row['cleaned_text'], max_length=1024)`**:\n",
    "    - For each row, the `split_text` function is called to divide the `cleaned_text` into manageable chunks of up to 1024 words.\n",
    "\n",
    "- **`for chunk in case_law_chunks:`**:\n",
    "    - This nested loop iterates through each chunk of the split text. For every chunk, a new dictionary is created containing the chunk of `cleaned_text` and its corresponding `summary` from the original row.\n",
    "\n",
    "- **`split_rows.append({...})`**:\n",
    "    - Each dictionary is appended to the `split_rows` list. This list will eventually hold all the individual chunks of text and their summaries.\n",
    "\n",
    "- **`split_df = pd.DataFrame(split_rows)`**:\n",
    "    - The `split_rows` list is converted into a new dataframe `split_df`. This dataframe contains the split chunks of text along with their summaries, making it easier to process smaller parts of each document during training.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Split the Text?\n",
    "\n",
    "Splitting the text into smaller chunks is a common practice when preparing text for training machine learning models, particularly in the case of large documents. Here’s why this is important:\n",
    "\n",
    "- **Token Limits**: Most models, like BART or other transformers, have a token limit (e.g., 1024 tokens). Documents longer than this limit cannot be processed as a whole, so splitting them ensures that each chunk fits within the model’s token constraints.\n",
    "- **Improved Training**: Training a model with smaller, more manageable chunks allows the model to focus on understanding and summarizing specific portions of text, leading to potentially better performance when handling large documents.\n",
    "- **Efficiency**: Breaking down documents into smaller pieces helps improve processing time, memory usage, and allows for batch processing, which can be more efficient during training.\n",
    "\n",
    "### Outcome:\n",
    "\n",
    "The code processes the dataset, splits long text documents into smaller chunks, and creates a new dataframe (`split_df`) that can be used for model training. Each chunk is paired with its corresponding summary, ensuring the model can learn from smaller, easily digestible pieces of information. This step is crucial for training on documents of varying lengths while adhering to the token limitations of the model.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "This block of code prepares the text data for training by splitting long documents into smaller chunks. This ensures that the model can process and summarize each piece of text without hitting token limits, improving both performance and efficiency during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1738184059850,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "Ct6bzdAh18WY",
    "outputId": "66e27dc2-bdee-43ef-ad5b-eaab9e34def0"
   },
   "outputs": [],
   "source": [
    "split_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Model\n",
    "### Explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1738184171167,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "fyouJPLfJ_YX"
   },
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_df = split_df.sample(frac=train_size, random_state=42)\n",
    "val_df = split_df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1738184175100,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "7Gp8Yy1KUeh7"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`train_size = 0.8`**:\n",
    "    - This defines the proportion of data to be used for training. In this case, 80% of the data will be used for training the model.\n",
    "\n",
    "- **`train_df = split_df.sample(frac=train_size, random_state=42)`**:\n",
    "    - This line randomly samples 80% of the rows from `split_df` to create the training set (`train_df`). The `random_state=42` ensures reproducibility of the results by setting a fixed random seed.\n",
    "\n",
    "- **`val_df = split_df.drop(train_df.index)`**:\n",
    "    - This creates the validation set by dropping the rows in `train_df` from the original dataframe `split_df`. The remaining 20% of the data will be used for validation.\n",
    "\n",
    "- **`train_dataset = Dataset.from_pandas(train_df)`** and **`val_dataset = Dataset.from_pandas(val_df)`**:\n",
    "    - These lines convert the training and validation dataframes (`train_df` and `val_df`) into Hugging Face `Dataset` objects. The `Dataset` class allows for easier manipulation and interaction with the data, especially when using Hugging Face's `transformers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1738184187968,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "wJINsUMcICsQ"
   },
   "outputs": [],
   "source": [
    "def tokenize_data(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['cleaned_text'],\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=True \n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['summary'],\n",
    "            max_length=150,\n",
    "            truncation=True,\n",
    "            padding=True \n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "44bb9980fe1842c5bedaddaa15c7a9dd",
      "5bd6ae4bca8e4ef8b58ed2e034aa3f8c",
      "7bf03ea3ed5341428f2764f1c80fd197",
      "252a2852728d45c696086098bf0fa8c1",
      "5fd3cd8ecdfe431f876988f25e800872",
      "d5a160be9acf4821ae63f7cc97a1cd6b",
      "1ea49547b50547579c8a57f6be2cce5e",
      "cc4186c4ec404004ae9192750d130087",
      "3692d0828d6d4695b5d21b17e1ae2fda",
      "01cf418beac3446f80d413268d229150",
      "330545bf6a1b4105b52ad86cbc29f696",
      "0e56f892f8cc4e1a8eada95848af91a7",
      "7f82626bfcaf4b35adaa7bc7bf9f97d9",
      "7dab8a4c37bc4c69a6d13186f4b53fbf",
      "8d5dacbe30c548b6a1ee7b2f67f04783",
      "f7efb4871ed2474792030f27e9027c44",
      "73c797ed46c048f496d3210acd21baca",
      "5805cc1bce1547dbb681c3a8f3d89706",
      "d33b9c205c744f2cbc0b9b64ed65a6e6",
      "f0f08600cd13406eb43dc366470f14d4",
      "86511236afa9409bb8f96c61bbe7dedb",
      "fbfdf16e9db6477c83d53d5043142a2c"
     ]
    },
    "executionInfo": {
     "elapsed": 6253,
     "status": "ok",
     "timestamp": 1738184198332,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "Zmt2_WCIIXM_",
    "outputId": "a465224d-9c3f-4b24-994b-f04cf3808591"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tokenize_data, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`def tokenize_data(examples):`**:\n",
    "    - This function is responsible for tokenizing the input text and its corresponding summary. Tokenization converts raw text into the numerical format (token IDs) required by the model.\n",
    "\n",
    "- **`model_inputs = tokenizer(...)`**:\n",
    "    - The input text (`cleaned_text`) is tokenized using the `tokenizer` object. The `max_length=1024` ensures that the input text is truncated if it exceeds 1024 tokens. The `truncation=True` option ensures that any text longer than 1024 tokens will be cut off, and `padding=True` applies dynamic padding to ensure that shorter sequences are padded to the same length.\n",
    "\n",
    "- **`with tokenizer.as_target_tokenizer():`**:\n",
    "    - This temporarily sets the tokenizer to behave as a \"target tokenizer,\" meaning it will be used to tokenize the target text (the summaries) rather than the input text.\n",
    "\n",
    "- **`labels = tokenizer(...)`**:\n",
    "    - The summaries (`examples['summary']`) are tokenized in a similar way to the input text, but with a smaller `max_length=150`, as the output (summary) is typically shorter. Again, `truncation=True` and `padding=True` are used to handle longer or shorter summaries.\n",
    "\n",
    "- **`model_inputs[\"labels\"] = labels[\"input_ids\"]`**:\n",
    "    - This assigns the tokenized summary (target) to the `labels` key in the `model_inputs` dictionary, which is the format expected by the BART model for training.\n",
    "\n",
    "- **`train_dataset = train_dataset.map(tokenize_data, batched=True)`** and **`val_dataset = val_dataset.map(tokenize_data, batched=True)`**:\n",
    "    - These lines apply the `tokenize_data` function to both the training and validation datasets using the `.map()` function. The `batched=True` option allows tokenization of multiple rows at once, which can speed up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1738184202255,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "ask9cXMwKIRJ"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)`**:\n",
    "    - The `DataCollatorForSeq2Seq` is a special data collator that dynamically pads the input and output sequences to the same length during batch processing. This is crucial when training a sequence-to-sequence model like BART because it ensures that each batch has consistent input/output lengths, improving efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Parameters\n",
    "### Explanation:\n",
    "#### Training Arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1738184204390,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "0oZWVteyH51k"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy = \"epoch\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`TrainingArguments`**:\n",
    "    - This class defines all the hyperparameters and configurations for training the model. These settings control how the model is trained, including aspects like batch size, learning rate, logging, and more.\n",
    "\n",
    "- **`output_dir=\"./results\"`**:\n",
    "    - This specifies the directory where the training results (e.g., model checkpoints, logs, etc.) will be saved.\n",
    "\n",
    "- **`per_device_train_batch_size=4`** and **`per_device_eval_batch_size=4`**:\n",
    "    - These define the batch size for training and evaluation, respectively. A batch size of 4 means that 4 samples are processed simultaneously per device (e.g., per GPU).\n",
    "\n",
    "- **`learning_rate=1e-5`**:\n",
    "    - The learning rate determines how much the model weights are adjusted during training with each update. A smaller learning rate (1e-5) ensures that the model learns gradually, reducing the risk of overshooting the optimal weights.\n",
    "\n",
    "- **`num_train_epochs=10`**:\n",
    "    - This specifies the number of epochs (full passes through the training dataset) during training. In this case, the model will be trained for 10 epochs.\n",
    "\n",
    "- **`weight_decay=0.01`**:\n",
    "    - Weight decay is a form of regularization to prevent overfitting by adding a penalty to the model's weights. A weight decay of 0.01 helps reduce the likelihood of overfitting by preventing the model from assigning too much importance to any one feature.\n",
    "\n",
    "- **`logging_dir=\"./logs\"`**:\n",
    "    - The directory where logs, including training metrics, will be saved. These logs can later be analyzed for insights into the training process.\n",
    "\n",
    "- **`logging_steps=10`**:\n",
    "    - This specifies how often (in terms of steps) the model should log its metrics. Every 10 steps, the model will log its current performance.\n",
    "\n",
    "- **`save_strategy = \"epoch\"`**:\n",
    "    - This tells the trainer to save model checkpoints at the end of each epoch. This ensures that a version of the model is saved after each full pass through the dataset.\n",
    "\n",
    "- **`eval_strategy = \"epoch\"`**:\n",
    "    - This instructs the trainer to evaluate the model on the validation dataset at the end of each epoch, allowing you to monitor its performance over time.\n",
    "\n",
    "- **`report_to=\"none\"`**:\n",
    "    - This disables reporting to third-party monitoring tools like TensorBoard or WandB, keeping the training process simpler by not sending the logs anywhere outside the local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1738184207496,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "hMzaePv2I4-V"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`Trainer`**:\n",
    "    - The `Trainer` class is a high-level API provided by Hugging Face to streamline the training process. It handles many of the complexities of training, evaluation, and saving checkpoints.\n",
    "\n",
    "- **`model=model`**:\n",
    "    - This specifies the model to be trained. In this case, it is the BART model loaded earlier (`BartForConditionalGeneration`).\n",
    "\n",
    "- **`args=training_args`**:\n",
    "    - The `args` parameter provides the trainer with all the training configurations defined in the `TrainingArguments` block.\n",
    "\n",
    "- **`train_dataset=train_dataset`** and **`eval_dataset=val_dataset`**:\n",
    "    - These are the datasets used for training and validation. The `train_dataset` contains the training data, while the `eval_dataset` holds the validation data. The trainer will use these datasets to fine-tune the model and evaluate its performance after each epoch.\n",
    "\n",
    "- **`data_collator=data_collator`**:\n",
    "    - The `data_collator` is responsible for dynamically padding and batching the sequences, ensuring that each batch has consistent input lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training:\n",
    "\n",
    "- **`trainer.train()`**:\n",
    "    - This starts the training process using the configurations, datasets, and model provided. The model will train for the specified number of epochs (10) and use the settings defined in the `TrainingArguments` to control the process. During training, the model will log its progress every 10 steps and save a checkpoint at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 519531,
     "status": "ok",
     "timestamp": 1738184729638,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "CBGH2qkoJ80B",
    "outputId": "46c435c1-4e18-4fa0-b5c0-00f80b7fa52b"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Case Law Summarizer Training Results\n",
    "\n",
    "The table shows the training and validation loss over 10 epochs for the case law summarizer model. Here's an evaluation of the results:\n",
    "\n",
    "| Epoch | Training Loss | Validation Loss |\n",
    "|-------|---------------|-----------------|\n",
    "| 1     | 1.320100      | 0.908481        |\n",
    "| 2     | 0.398700      | 0.285914        |\n",
    "| 3     | 0.141100      | 0.127164        |\n",
    "| 4     | 0.080600      | 0.084278        |\n",
    "| 5     | 0.031400      | 0.067373        |\n",
    "| 6     | 0.043000      | 0.074902        |\n",
    "| 7     | 0.028400      | 0.066691        |\n",
    "| 8     | 0.016000      | 0.075075        |\n",
    "| 9     | 0.011100      | 0.070565        |\n",
    "| 10    | 0.009100      | 0.068837        |\n",
    "\n",
    "#### Epoch 1: \n",
    "- **Training Loss:** 1.320100\n",
    "- **Validation Loss:** 0.908481\n",
    "  - At the start, both training and validation loss are relatively high. This indicates that the model has not yet learned how to generate accurate summaries, but this is expected in the first epoch as the model is still adjusting its weights.\n",
    "\n",
    "#### Epoch 2: \n",
    "- **Training Loss:** 0.398700\n",
    "- **Validation Loss:** 0.285914\n",
    "  - Significant improvement is seen in both losses, especially in the validation loss, which drops by more than half. This suggests the model has started learning effectively and is beginning to generalize better to unseen data.\n",
    "\n",
    "#### Epoch 3:\n",
    "- **Training Loss:** 0.141100\n",
    "- **Validation Loss:** 0.127164\n",
    "  - Both losses continue to drop, with validation loss nearing 0.1. The model's ability to summarize case law accurately is improving, and overfitting has not yet set in, as validation loss still decreases.\n",
    "\n",
    "#### Epoch 4-5:\n",
    "- **Training Loss:** 0.080600 → 0.031400\n",
    "- **Validation Loss:** 0.084278 → 0.067373\n",
    "  - Training loss decreases at a faster rate than validation loss. The validation loss shows a steady decline, indicating continued improvement in generalization to unseen data.\n",
    "\n",
    "#### Epoch 6-10:\n",
    "- **Training Loss:** 0.043000 → 0.009100\n",
    "- **Validation Loss:** 0.074902 → 0.068837\n",
    "  - Although training loss continues to decrease, validation loss starts to fluctuate slightly around 0.07. This could indicate that the model has reached a point of diminishing returns in learning from the data. The slight increase in validation loss (epochs 6 and 8) might suggest early signs of overfitting.\n",
    "\n",
    "### Key Observations:\n",
    "1. **Steady Improvement:** The training and validation losses consistently decreased, especially during the first five epochs, indicating strong learning behavior.\n",
    "2. **Early Overfitting Signs:** The small fluctuation in validation loss towards the end suggests that the model might have started overfitting slightly, even as training loss continued to improve. However, the overfitting is not severe, as validation loss remains low.\n",
    "3. **Training Success:** Overall, the model has learned to effectively summarize case law, with both losses reaching very low values by epoch 10.\n",
    "\n",
    "### Next Steps (Lab 3):\n",
    "1. **Hyperparameter Tuning:** To further enhance performance and reduce the small overfitting signs, hyperparameter tuning will be crucial. Adjusting parameters like learning rate, batch size, and regularization terms may help stabilize validation loss.\n",
    "2. **Detoxification:** As the model is designed for case law, detoxifying the model (e.g., removing biases or inappropriate content) will be critical to ensure it produces ethical and balanced summaries.\n",
    "\n",
    "Lab 3 will focus on these improvements to refine the model’s performance and enhance its robustness for legal summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8410,
     "status": "ok",
     "timestamp": 1738184886385,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "hVJN0biDKBvu",
    "outputId": "9e65cd94-c407-467f-d87f-ffff106357ad"
   },
   "outputs": [],
   "source": [
    "save_path = \"/content/drive/MyDrive/fine_tuned_bart_best\"\n",
    "\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1738091293066,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "SL9RgfwuLe05"
   },
   "outputs": [],
   "source": [
    "model_path = \"./fine_tuned_bart_best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91490,
     "status": "ok",
     "timestamp": 1738091675210,
     "user": {
      "displayName": "Bashaar Dhoot",
      "userId": "04066267376811464441"
     },
     "user_tz": 0
    },
    "id": "KI5zGX06VKhR",
    "outputId": "0be9aec9-bd1d-4c90-b1d2-69d71517bdb3"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/fine_tuned_bart_best.zip /content/fine_tuned_bart_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZ3W70eq5bry"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNFa1sDYByjuClUYqzXhxl7",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
