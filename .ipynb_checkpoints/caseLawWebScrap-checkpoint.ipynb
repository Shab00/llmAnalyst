{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **National Archives Web Scrapper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBFSai4sLHLy",
    "outputId": "9efce259-bb7c-4ba6-9e37-b2dc43e552cf"
   },
   "source": [
    "! pip install feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Modules Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucRU4mHUPvPd",
    "outputId": "ffecb8db-60d4-4db5-c798-b489a57806b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "import requests\n",
    "import feedparser\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **IPython**\n",
    "   - **`get_ipython`**: Provides an interface to interact with the IPython runtime environment. It allows access to IPython's configuration and utilities for working with IPython sessions.\n",
    "   - **`display`**: A function used to display rich representations of objects (like images, HTML, and more) in IPython environments such as Jupyter Notebooks.\n",
    "\n",
    "2. **`requests`**\n",
    "   - A Python library for making HTTP requests. It simplifies sending HTTP requests to interact with web services or fetch data from URLs.\n",
    "   - Example use: Sending a GET or POST request to a web API or downloading data from a URL.\n",
    "\n",
    "3. **`feedparser`**\n",
    "   - A module used for parsing RSS and Atom feeds. It allows extracting data like blog posts, articles, or news from XML feeds provided by websites.\n",
    "   - Example use: Reading and parsing RSS feeds from news websites.\n",
    "\n",
    "4. **`os`**\n",
    "   - A built-in Python library for interacting with the operating system. It provides functionalities for working with directories, files, environment variables, and other OS-level operations.\n",
    "   - Example use: Accessing file paths or managing directories programmatically.\n",
    "\n",
    "5. **`time`**\n",
    "   - A built-in Python module that provides various time-related functions, such as pausing execution (`sleep`), getting the current time (`time`), or measuring elapsed time.\n",
    "   - Example use: Implementing delays or time-related calculations.\n",
    "\n",
    "6. **`bs4` (BeautifulSoup)**\n",
    "   - A popular library used for parsing HTML and XML documents. BeautifulSoup makes it easy to navigate and extract data from web pages by using tag and attribute searches.\n",
    "   - Example use: Scraping information from web pages by parsing the HTML content.\n",
    "\n",
    "7. **`xml.etree.ElementTree` (ET)**\n",
    "   - A built-in Python module for parsing and creating XML documents. It provides a tree structure for manipulating XML data and extracting specific elements.\n",
    "   - Example use: Reading, parsing, and modifying XML documents.\n",
    "\n",
    "8. **`csv`**\n",
    "   - A Python module for handling CSV (Comma-Separated Values) files. It provides methods to read from and write to CSV files, making it easy to work with structured tabular data.\n",
    "   - Example use: Reading a dataset from a CSV file or writing data to a CSV file for storage.\n",
    "\n",
    "9. **`google.colab.drive` (Colab Drive)\n",
    "   - **`drive.mount()`**: A function to mount Google Drive in a Colab environment, allowing access to files stored in your Google Drive. This is commonly used to load or save data from Google Drive while working in Colab.\n",
    "   - Example use: Mounting Google Drive to read and save files in Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cK1nRGAdM3VT"
   },
   "outputs": [],
   "source": [
    "# Updated Atom feed URL\n",
    "feed_url = \"https://caselaw.nationalarchives.gov.uk/atom.xml?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Wg2YtE5UMGBJ"
   },
   "outputs": [],
   "source": [
    "# Headers for requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `scrape_case_content(topic_url)`\n",
    "\n",
    "#### Purpose:\n",
    "This function scrapes the content of a web page (specifically paragraphs of text) and stores it in an XML file. It is designed to scrape case law text from the provided `topic_url`, save the content into an XML structure, and append it to an existing XML file stored in Google Drive.\n",
    "\n",
    "#### Parameters:\n",
    "- **`topic_url`**: A string representing the URL of the webpage to be scraped for case law content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RWPL3rjgaLU5"
   },
   "outputs": [],
   "source": [
    "def scrape_case_content(topic_url):\n",
    "    try:\n",
    "        response = requests.get(topic_url, headers=headers)\n",
    "        topic_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        sections = topic_soup.find_all('p')\n",
    "\n",
    "        case_element = ET.Element(\"case\")\n",
    "        for section in sections:\n",
    "            text_element = ET.SubElement(case_element, \"text\")\n",
    "            text_element.text = section.get_text()\n",
    "\n",
    "        try:\n",
    "            tree = ET.parse('/content/drive/MyDrive/all_case_data.xml')\n",
    "            root = tree.getroot()\n",
    "            root.append(case_element)\n",
    "        except FileNotFoundError:\n",
    "            root = ET.Element(\"cases\")\n",
    "            root.append(case_element)\n",
    "            tree = ET.ElementTree(root)\n",
    "\n",
    "        tree.write('/content/drive/MyDrive/all_case_data.xml', encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "        print(f\"Scraped content from {topic_url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {topic_url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps:\n",
    "1. **Make HTTP Request:**\n",
    "   - The function uses the `requests.get()` method to send an HTTP request to the given `topic_url`. The response is fetched with the headers defined earlier in the code.\n",
    "   \n",
    "2. **Parse HTML with BeautifulSoup:**\n",
    "   - The response content is parsed using BeautifulSoup, specifying the `'html.parser'` as the parsing engine.\n",
    "   - The function looks for all `<p>` tags in the HTML (which typically represent paragraphs) using `find_all('p')`.\n",
    "\n",
    "3. **Create XML Elements:**\n",
    "   - A new XML element named `<case>` is created using `xml.etree.ElementTree`.\n",
    "   - For each paragraph found in the page (`sections`), a new `<text>` sub-element is created within the `<case>` element. The text content from each paragraph is added as the value of the `<text>` element.\n",
    "\n",
    "4. **Append to or Create XML File:**\n",
    "   - The function attempts to open and parse an existing XML file (`all_case_data.xml`) located in Google Drive.\n",
    "     - If the file is found, it retrieves the root element and appends the new `<case>` element.\n",
    "     - If the file is not found (i.e., `FileNotFoundError`), a new XML file is created with a root element `<cases>`, and the new `<case>` element is added.\n",
    "   \n",
    "5. **Write XML File:**\n",
    "   - The XML tree is written back to the `all_case_data.xml` file in Google Drive, with UTF-8 encoding and an XML declaration.\n",
    "\n",
    "6. **Error Handling:**\n",
    "   - If an error occurs during the scraping process (e.g., network issues or parsing errors), the exception is caught, and an error message is printed to indicate that the scraping failed for the given URL.\n",
    "\n",
    "#### Output:\n",
    "- The function prints a message indicating the successful scraping of content from the `topic_url`. In the case of an error, an error message is displayed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet: Fetch and Scrape Case Content from RSS Feed\n",
    "\n",
    "#### Purpose:\n",
    "This code snippet retrieves legal case URLs from an RSS feed, then scrapes the content of each case page and stores it in an XML file using the previously defined `scrape_case_content()` function. A delay is added between requests to avoid overloading the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3rrIz-BMqWv",
    "outputId": "09ee9c25-c362-485e-acc0-6b2dbb08fa07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/kb/2025/138\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/ch/2025/135\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/admin/2025/123\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/comm/2025/140\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewca/crim/2025/52\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/ch/2025/136\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewca/crim/2025/51\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/admin/2025/137\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukftt/tc/2025/66\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/73\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/admin/2025/113\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/tcc/2025/100\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/kb/2025/127\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewca/civ/2025/42\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukftt/tc/2025/65\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukftt/tc/2025/67\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewfc/b/2025/10\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/comm/2025/32\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/61\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/kb/2025/134\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewca/civ/2025/41\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/kb/2025/106\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/admin/2025/128\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/eat/2025/10\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewca/civ/2025/39\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/scco/2025/126\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/comm/2025/120\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/kb/2025/111\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/kb/2025/109\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukut/lc/2024/19\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/ch/2025/104\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewca/crim/2025/38\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewca/civ/2025/36\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/62\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewca/civ/2025/40\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/tcc/2025/108\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/scco/2025/125\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/tcc/2025/105\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/scco/2025/118\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/63\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukut/tcc/2025/28\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/admin/2025/101\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/tcc/2025/90\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/kb/2025/99\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukftt/tc/2025/68\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukftt/grc/2025/54\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/ch/2025/85\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewhc/admin/2025/87\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ewca/crim/2025/25\n",
      "Scraped content from https://caselaw.nationalarchives.gov.uk/ukftt/tc/2025/69\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(feed_url, headers=headers)\n",
    "feed = feedparser.parse(response.content)\n",
    "\n",
    "judgment_links = [entry.link for entry in feed.entries]\n",
    "\n",
    "for link in judgment_links:\n",
    "    scrape_case_content(link)  \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps:\n",
    "\n",
    "1. **Fetch the RSS Feed:**\n",
    "   - The `requests.get()` method is used to send an HTTP request to the provided `feed_url`. The response is fetched, and the `headers` parameter is passed to simulate a legitimate web browser request.\n",
    "   - The content of the response (which is the RSS feed) is passed to the `feedparser.parse()` function to parse the RSS feed into a structured format.\n",
    "\n",
    "2. **Extract Judgment Links:**\n",
    "   - The `feed.entries` attribute contains the parsed RSS feed entries. A list comprehension is used to extract the `link` attribute from each entry, which points to the specific case judgment page.\n",
    "   - These links are stored in the `judgment_links` list.\n",
    "\n",
    "3. **Scrape Case Content:**\n",
    "   - The code iterates through the list of `judgment_links`, and for each link:\n",
    "     - It calls the `scrape_case_content(link)` function to scrape and save the case content from the respective URL.\n",
    "     - A delay of 5 seconds (`time.sleep(5)`) is added between each request to prevent overwhelming the server with rapid successive requests, ensuring responsible scraping behavior.\n",
    "\n",
    "#### Output:\n",
    "- The content of each case linked in the RSS feed is scraped and appended to an XML file located in Google Drive.\n",
    "- A 5-second delay is introduced between requests to avoid hitting server rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JlM70a0pSldY",
    "outputId": "9819a5f0-4e4c-4d3a-b547-a929d477d3a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of case laws downloaded: 100\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tree = ET.parse('/content/drive/MyDrive/all_case_data.xml')\n",
    "    root = tree.getroot()\n",
    "    case_count = len(root.findall('case'))\n",
    "    print(f\"Total number of case laws downloaded: {case_count}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"XML file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBmbHheBSm8q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
